{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Neural Networks are one of the most used tools in machine learning. Basically, what we are doing is putting together many perceptrons into layers for a more complex system. Here, I will be using an input layer (64 nodes) -> a hidden layer (5 nodes) -> output layer (10 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 65)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "\n",
    "# Adding a bias term\n",
    "temp = []\n",
    "for i in range(len(x)):\n",
    "    temp.append(x[i])\n",
    "    temp[i] = np.append(temp[i], 1)\n",
    "    temp[i] = (temp[i] - np.mean(temp[i])) / np.std(temp[i])\n",
    "\n",
    "x = np.array(temp)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87916847, -0.87916847,  0.08940696,  1.63912766,  0.86426731,\n",
       "       -0.68545339, -0.87916847, -0.87916847, -0.87916847, -0.87916847,\n",
       "        1.63912766,  2.02655784,  1.0579824 ,  2.02655784,  0.08940696,\n",
       "       -0.87916847, -0.87916847, -0.29802321,  2.02655784, -0.4917383 ,\n",
       "       -0.87916847,  1.25169749,  0.67055223, -0.87916847, -0.87916847,\n",
       "       -0.10430812,  1.44541258, -0.87916847, -0.87916847,  0.67055223,\n",
       "        0.67055223, -0.87916847, -0.87916847,  0.08940696,  0.67055223,\n",
       "       -0.87916847, -0.87916847,  0.86426731,  0.67055223, -0.87916847,\n",
       "       -0.87916847, -0.10430812,  1.25169749, -0.87916847, -0.68545339,\n",
       "        1.44541258,  0.47683714, -0.87916847, -0.87916847, -0.4917383 ,\n",
       "        1.83284275,  0.08940696,  1.0579824 ,  1.44541258, -0.87916847,\n",
       "       -0.87916847, -0.87916847, -0.87916847,  0.28312205,  1.63912766,\n",
       "        1.0579824 , -0.87916847, -0.87916847, -0.87916847, -0.68545339])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>0.089407</td>\n",
       "      <td>1.639128</td>\n",
       "      <td>0.864267</td>\n",
       "      <td>-0.685453</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>0.283122</td>\n",
       "      <td>1.639128</td>\n",
       "      <td>1.057982</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.879168</td>\n",
       "      <td>-0.685453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>1.113781</td>\n",
       "      <td>1.269137</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>0.958425</td>\n",
       "      <td>1.735204</td>\n",
       "      <td>0.803070</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.750488</td>\n",
       "      <td>-0.595132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.208461</td>\n",
       "      <td>1.545065</td>\n",
       "      <td>1.066830</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.367873</td>\n",
       "      <td>0.907419</td>\n",
       "      <td>1.704476</td>\n",
       "      <td>0.588596</td>\n",
       "      <td>-0.846107</td>\n",
       "      <td>-0.686695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>0.539405</td>\n",
       "      <td>2.039353</td>\n",
       "      <td>1.664366</td>\n",
       "      <td>-0.585557</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>0.726898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>0.539405</td>\n",
       "      <td>1.664366</td>\n",
       "      <td>1.664366</td>\n",
       "      <td>0.914392</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.773050</td>\n",
       "      <td>-0.585557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.532369</td>\n",
       "      <td>1.251341</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.353998</td>\n",
       "      <td>2.143196</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.710740</td>\n",
       "      <td>-0.532369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.879168 -0.879168  0.089407  1.639128  0.864267 -0.685453 -0.879168   \n",
       "1 -0.750488 -0.750488 -0.750488  1.113781  1.269137  0.026291 -0.750488   \n",
       "2 -0.846107 -0.846107 -0.846107 -0.208461  1.545065  1.066830 -0.846107   \n",
       "3 -0.773050 -0.773050  0.539405  2.039353  1.664366 -0.585557 -0.773050   \n",
       "4 -0.710740 -0.710740 -0.710740 -0.532369  1.251341 -0.710740 -0.710740   \n",
       "\n",
       "         7         8         9   ...        55        56        57        58  \\\n",
       "0 -0.879168 -0.879168 -0.879168  ... -0.879168 -0.879168 -0.879168  0.283122   \n",
       "1 -0.750488 -0.750488 -0.750488  ... -0.750488 -0.750488 -0.750488 -0.750488   \n",
       "2 -0.846107 -0.846107 -0.846107  ... -0.846107 -0.846107 -0.846107 -0.846107   \n",
       "3 -0.773050 -0.773050  0.726898  ... -0.773050 -0.773050 -0.773050  0.539405   \n",
       "4 -0.710740 -0.710740 -0.710740  ... -0.710740 -0.710740 -0.710740 -0.710740   \n",
       "\n",
       "         59        60        61        62        63        64  \n",
       "0  1.639128  1.057982 -0.879168 -0.879168 -0.879168 -0.685453  \n",
       "1  0.958425  1.735204  0.803070 -0.750488 -0.750488 -0.595132  \n",
       "2 -0.367873  0.907419  1.704476  0.588596 -0.846107 -0.686695  \n",
       "3  1.664366  1.664366  0.914392 -0.773050 -0.773050 -0.585557  \n",
       "4 -0.353998  2.143196  0.002744 -0.710740 -0.710740 -0.532369  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that since each row is an 8x8 picture, it has 64 columns in it\n",
    "# Each row is an image\n",
    "pd.DataFrame(x).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = digits.target\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10,15)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x[range(1500)]\n",
    "train_y = y[range(1500)]\n",
    "\n",
    "test_x = x[range(1500, 1797)]\n",
    "tesy_y = y[range(1500, 1797)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z):\n",
    "    \"\"\"\n",
    "    Sigmoid Function that takes in a number, z, and outputs\n",
    "    the sigmoid function evaluated at z, which is in the\n",
    "    interval [0, 1]\n",
    "    \"\"\"\n",
    "    return (1 + np.exp(-z))**(-1)\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Loss function that takes in two arrays of equal length,\n",
    "    y and y_hat, and calculates the sum of squares loss between\n",
    "    the two\n",
    "    \"\"\"\n",
    "    return sum((y - y_hat)**2)\n",
    "\n",
    "def gradient_w2(y_hat, y, hidden_layer, w2, α):\n",
    "    \"\"\"\n",
    "    Function to calculate the gradient for any one of the set of\n",
    "    weights between the hidden layer and one output node\n",
    "    INPUTS:\n",
    "        - y_hat, the calculation of output node k\n",
    "        - y, the target value for output node k\n",
    "        - w2, the array of weights that go from the hidden layer\n",
    "            to one of the output nodes k\n",
    "        - α, the learning rate\n",
    "    OUTPUTS:\n",
    "        - w2_new, an array of new weights for the weights\n",
    "                between layer two and output node k\n",
    "    \"\"\"\n",
    "    gradient = []\n",
    "    for j in range(len(w2)):\n",
    "        gradient.append(2 * (y - y_hat) * y_hat * (1 - y_hat) * hidden_layer[j])\n",
    "    gradient = np.array(gradient)\n",
    "    \n",
    "    w2_new = w2 - (α * gradient)\n",
    "    return w2_new\n",
    "    \n",
    "def gradient_w1(y_vals, y_hats, w2_j, node_j, x, w1, α):\n",
    "    \"\"\"\n",
    "    Function to calculate the gradient for any one of the set of\n",
    "    weights between the input layer and one hidden layer node\n",
    "    INPUTS:\n",
    "        - y_vals, the target outputs for the output layer\n",
    "        - y_hats, the calculated values for the output layer\n",
    "        - w2_j, an array of each of the weights from node j\n",
    "                to the outputs\n",
    "        - node_j, the value of node j of the hidden layer\n",
    "        - x, the input values used in the feed forward algorithm\n",
    "        - w1, a set of weights from all the input nodes to node j\n",
    "             in the hidden layer\n",
    "        - α, the learning rate with which to scale the change in weights\n",
    "        \n",
    "    OUTPUTS:\n",
    "        - w1_new, an array of new weights \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for k in range(10):\n",
    "        total += (y_vals[k] - y_hats[k]) * y_hats[k] * (1 - y_hats[k]) * w2_j[k]\n",
    "        \n",
    "    gradient = total * 2 * x\n",
    "     \n",
    "    w1_new = w1 - (α * gradient)\n",
    "    return w1_new\n",
    "\n",
    "def feed_forward(x, w1, w2):\n",
    "    \"\"\"\n",
    "    Function to feed forward the array, x, and give outputs\n",
    "    INPUTS:\n",
    "        - x, an array of length 64, the data to be passed through\n",
    "        - w1, the matrix holding all the weights from the input\n",
    "            layer to the hidden layer\n",
    "        - w2, the matrix holding all the weights from the hidden layer\n",
    "            to the output layer\n",
    "    OUTPUTS:\n",
    "        - outputs, an array of length 10\n",
    "        - hidden_layer, an array of length 5\n",
    "    \"\"\"\n",
    "    hidden_layer = []\n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        hidden_layer.append(np.dot(w1[:,i], x))\n",
    "    hidden_layer.append(1)\n",
    "        \n",
    "    for i in range(10):\n",
    "        z = np.dot(w2[:,i], np.array(hidden_layer))\n",
    "        outputs.append(σ(z))\n",
    "        \n",
    "    return {'o':outputs, 'hl':hidden_layer}\n",
    "\n",
    "def NeuralNet(x, y, weights1, weights2, num_iterations=100, α=0.25):\n",
    "    \"\"\"\n",
    "    Function for optimizing the weights for the 3-layer Neural Network\n",
    "    INPUTS:\n",
    "        - x, the input data matrix with size m x n, where each row \n",
    "            is an observations\n",
    "        - y, an array of length m with all the correct classifications\n",
    "            of the data in x\n",
    "        - weights1, a matrix of size 65 x 6, where each column is a set of weights\n",
    "            corresponding to one of the hidden layer nodes\n",
    "        - weights2, a matrix of size 6 x 10, where each column is a set of weights\n",
    "            corresponding from the hidden layer to one of the output nodes\n",
    "        - num_iterations, an integer, the maximum number of iterations\n",
    "                        the algorithm will perform before stopping\n",
    "        - α, the learning rate for the algorithm\n",
    "    OUTPUTS:\n",
    "        - new_w1, the optimized weights for the first layer\n",
    "        - new_w2, the optimized weights for the second layer\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    n = len(x[0])\n",
    "    j = 0\n",
    "    \n",
    "    w1 = weights1\n",
    "    w2 = weights2\n",
    "    error = []\n",
    "    while (j <= num_iterations):\n",
    "        j+=1\n",
    "        \n",
    "        # Feed Forward\n",
    "        for i in range(m):\n",
    "            obs = x[i]\n",
    "            target = y[i]\n",
    "            \n",
    "            ff = feed_forward(x[i], w1, w2)\n",
    "            y_hats = ff['o']\n",
    "            hidden_layer = ff['hl']\n",
    "            guess = np.argmax(np.array(y_hats))\n",
    "    \n",
    "            # Back Propagation\n",
    "            if (guess != y[i]):\n",
    "                \n",
    "                target_y = np.zeros(10)\n",
    "                target_y[target] = 1\n",
    "                \n",
    "                # Updating the weights between the hidden layer and the output layer\n",
    "                for k in range(10):\n",
    "                    w2[:,k] = gradient_w2(y_hats[k], target_y[k], hidden_layer, w2[:,k], α)\n",
    "        \n",
    "                # Updating the weights between the input layer and the hidden layer\n",
    "                for k in range(6):\n",
    "                    w1[:,k] = gradient_w1(target_y, y_hats, w2[k,:], hidden_layer[k], obs, w1[:,k], α)\n",
    "            \n",
    "    \n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58000364, 0.46549681, 0.36843499, 0.11872796, 0.31581387,\n",
       "       0.21159597])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights between the input layer and layer 2\n",
    "weights1 = np.random.random((64+1)*6).reshape(65, 6)\n",
    "\n",
    "# Weights between layer 2 and the output layer\n",
    "weights2 = np.random.random((5+1)*10).reshape(6, 10)\n",
    "weights2[:,0]\n",
    "# This would represent all of the weights from the first node in the \n",
    "# hidden layer to the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = NeuralNet(train_x, train_y, weights1, weights2, num_iterations=25, α=.005)\n",
    "w1 = weights[0]\n",
    "w2 = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.126118417001749e-243,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.320159698518224e-165,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(w1, w2, x):\n",
    "    \"\"\"\n",
    "    Function to predict what number observation x\n",
    "    is based on the given weights w1 and w2\n",
    "    \"\"\"\n",
    "    outputs = feed_forward(x, w1, w2)['o']\n",
    "    return outputs, np.argmax(outputs)\n",
    "\n",
    "i = np.random.randint(1500, 1797)\n",
    "x_i = x[i]\n",
    "predict(w1, w2, x_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  6.126118417001749e-243,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  4.320159698518224e-165,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'hl': [-1.3652966952199872,\n",
       "  -4.91727547312207,\n",
       "  -9.087100200000483,\n",
       "  134.45068356181875,\n",
       "  476.25998680642874,\n",
       "  1]}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(x_i, weights1, weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.89124631, 1.86769133, 2.35137273, 2.2345828 , 1.44321127,\n",
       "       1.69969278])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
